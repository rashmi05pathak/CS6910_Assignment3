{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3_Seq2Seq.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rashmi05pathak/CS6910_Assignment3/blob/main/Assignment3_Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import pathlib"
      ],
      "metadata": {
        "id": "svLneXC3dJoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Downloading the Dakshina dataset"
      ],
      "metadata": {
        "id": "uLIo093sc07m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading dakshina dataset\n",
        "!yes | wget \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\"\n",
        "# Unzipping dataset\n",
        "!yes | tar xopf dakshina_dataset_v1.0.tar"
      ],
      "metadata": {
        "id": "74oMB5ugcrSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "Ku7vMnRMZF25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3KQEek0Yl0k"
      },
      "source": [
        "2. Processing of the **Dakshina** dataset\n",
        "reference used : https://colab.research.google.com/drive/1rqHhdPbOeqlP_X6AW__4P37fXoyCnl9x#scrollTo=m5luH6y4Mvgi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wtNj_IqYrW5"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import pathlib\n",
        "\n",
        "\n",
        "class DataProcessing():\n",
        "\n",
        "    def __init__(self, DATAPATH, source_lang = 'en', target_lang = \"hi\"):\n",
        "    \n",
        "        self.source_lang = source_lang\n",
        "        self.target_lang = target_lang\n",
        "    \n",
        "        self.trainpath = os.path.join(DATAPATH, target_lang, \"lexicons\", target_lang+\".translit.sampled.train.tsv\")\n",
        "        self.valpath = os.path.join(DATAPATH, target_lang, \"lexicons\", target_lang+\".translit.sampled.dev.tsv\")\n",
        "        self.testpath = os.path.join(DATAPATH, target_lang, \"lexicons\", target_lang+\".translit.sampled.test.tsv\")\n",
        "        self.train = pd.read_csv(\n",
        "            self.trainpath,\n",
        "            sep=\"\\t\",\n",
        "            names=[\"tgt\", \"src\", \"count\"],\n",
        "        )\n",
        "        self.val = pd.read_csv(\n",
        "            self.valpath,\n",
        "            sep=\"\\t\",\n",
        "            names=[\"tgt\", \"src\", \"count\"],\n",
        "        )\n",
        "        self.test = pd.read_csv(\n",
        "            self.testpath,\n",
        "            sep=\"\\t\",\n",
        "            names=[\"tgt\", \"src\", \"count\"],\n",
        "        )\n",
        "\n",
        "        # create train data\n",
        "        self.train_data = self.preprocess(self.train[\"src\"].to_list(), self.train[\"tgt\"].to_list())\n",
        "        (\n",
        "            self.train_encoder_input,\n",
        "            self.train_decoder_input,\n",
        "            self.train_decoder_target,\n",
        "            self.source_vocab,\n",
        "            self.target_vocab,\n",
        "        ) = self.train_data\n",
        "        self.source_char2int, self.source_int2char = self.source_vocab\n",
        "        self.target_char2int, self.target_int2char = self.target_vocab\n",
        "\n",
        "        # create val data (only encode function suffices as the dictionary lookup should be kep the same.\n",
        "        self.val_data = self.encode(\n",
        "            self.val[\"src\"].to_list(),\n",
        "            self.val[\"tgt\"].to_list(),\n",
        "            list(self.source_char2int.keys()),\n",
        "            list(self.target_char2int.keys()),\n",
        "            source_char2int=self.source_char2int,\n",
        "            target_char2int=self.target_char2int,\n",
        "        )\n",
        "        self.val_encoder_input, self.val_decoder_input, self.val_decoder_target = self.val_data\n",
        "        self.source_char2int, self.source_int2char = self.source_vocab\n",
        "        self.target_char2int, self.target_int2char = self.target_vocab\n",
        "\n",
        "        # create test data\n",
        "        self.test_data = self.encode(\n",
        "            self.test[\"src\"].to_list(),\n",
        "            self.test[\"tgt\"].to_list(),\n",
        "            list(self.source_char2int.keys()),\n",
        "            list(self.target_char2int.keys()),\n",
        "            source_char2int=self.source_char2int,\n",
        "            target_char2int=self.target_char2int,\n",
        "        )\n",
        "        self.test_encoder_input, self.test_decoder_input, self.test_decoder_target = self.test_data\n",
        "        self.source_char2int, self.source_int2char = self.source_vocab\n",
        "        self.target_char2int, self.target_int2char = self.target_vocab\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    def dictionary_lookup(self, vocab):\n",
        "        char2int = dict([(char, i) for i, char in enumerate(vocab)])\n",
        "        int2char = dict((i, char) for char, i in char2int.items())\n",
        "        return char2int, int2char\n",
        "\n",
        "\n",
        "    def encode(self, source, target, source_chars, target_chars, source_char2int=None, target_char2int=None):\n",
        "        num_encoder_tokens = len(source_chars)\n",
        "        num_decoder_tokens = len(target_chars)\n",
        "        max_source_length = max([len(txt) for txt in source])\n",
        "        max_target_length = max([len(txt) for txt in target])\n",
        "\n",
        "        source_vocab, target_vocab = None, None\n",
        "        if source_char2int == None and target_char2int == None:\n",
        "            print(\"Generating the dictionary lookups for character to integer mapping and back\")\n",
        "            source_char2int, source_int2char = self.dictionary_lookup(source_chars)\n",
        "            target_char2int, target_int2char = self.dictionary_lookup(target_chars)\n",
        "\n",
        "            source_vocab = (source_char2int, source_int2char)\n",
        "            target_vocab = (target_char2int, target_int2char)\n",
        "\n",
        "        encoder_input_data = np.zeros(\n",
        "            (len(source), max_source_length, num_encoder_tokens), dtype=\"float32\"\n",
        "        )\n",
        "        decoder_input_data = np.zeros(\n",
        "            (len(source), max_target_length, num_decoder_tokens), dtype=\"float32\"\n",
        "        )\n",
        "        decoder_target_data = np.zeros(\n",
        "            (len(source), max_target_length, num_decoder_tokens), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        for i, (input_text, target_text) in enumerate(zip(source, target)):\n",
        "            for t, char in enumerate(input_text):\n",
        "                encoder_input_data[i, t, source_char2int[char]] = 1.0\n",
        "            encoder_input_data[i, t + 1 :, source_char2int[\" \"]] = 1.0\n",
        "            for t, char in enumerate(target_text):\n",
        "                # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "                decoder_input_data[i, t, target_char2int[char]] = 1.0\n",
        "                if t > 0:\n",
        "                    # decoder_target_data will be ahead by one timestep\n",
        "                    # and will not include the start character.\n",
        "                    decoder_target_data[i, t - 1, target_char2int[char]] = 1.0\n",
        "            decoder_input_data[i, t + 1 :, target_char2int[\" \"]] = 1.0\n",
        "            decoder_target_data[i, t:, target_char2int[\" \"]] = 1.0\n",
        "        if source_vocab != None and target_vocab != None:\n",
        "            return (\n",
        "                encoder_input_data,\n",
        "                decoder_input_data,\n",
        "                decoder_target_data,\n",
        "                source_vocab,\n",
        "                target_vocab,\n",
        "            )\n",
        "        else:\n",
        "            return encoder_input_data, decoder_input_data, decoder_target_data\n",
        "\n",
        "\n",
        "    def preprocess(self, source , target):\n",
        "        source_chars = set()\n",
        "        target_chars = set()\n",
        "\n",
        "        source = [str(x) for x in source]\n",
        "        target = [str(x) for x in target]\n",
        "\n",
        "        source_words = []\n",
        "        target_words = []\n",
        "        for src, tgt in zip(source, target):\n",
        "            tgt = \"\\t\" + tgt + \"\\n\"\n",
        "            source_words.append(src)\n",
        "            target_words.append(tgt)\n",
        "            for char in src:\n",
        "                if char not in source_chars:\n",
        "                    source_chars.add(char)\n",
        "            for char in tgt:\n",
        "                if char not in target_chars:\n",
        "                    target_chars.add(char)\n",
        "\n",
        "        source_chars = sorted(list(source_chars))\n",
        "        target_chars = sorted(list(target_chars))\n",
        "\n",
        "        #The space needs to be appended so that the encode function doesn't throw errors\n",
        "        source_chars.append(\" \")\n",
        "        target_chars.append(\" \")\n",
        "\n",
        "        num_encoder_tokens = len(source_chars)\n",
        "        num_decoder_tokens = len(target_chars)\n",
        "        max_source_length = max([len(txt) for txt in source_words])\n",
        "        max_target_length = max([len(txt) for txt in target_words])\n",
        "\n",
        "        print(\"Number of samples:\", len(source))\n",
        "        print(\"Source Vocab length:\", num_encoder_tokens)\n",
        "        print(\"Target Vocab length:\", num_decoder_tokens)\n",
        "        print(\"Max sequence length for inputs:\", max_source_length)\n",
        "        print(\"Max sequence length for outputs:\", max_target_length)\n",
        "\n",
        "        return self.encode(source_words, target_words, source_chars, target_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYb2pXHTZKc0"
      },
      "source": [
        "### 2.2 Processing the data\n",
        "\n",
        "Default input language is English and output language is Hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-kjn6OdZVkz"
      },
      "source": [
        "\n",
        "DATAPATH = \"./dakshina_dataset_v1.0\"\n",
        "\n",
        "#By default source language is English and target lang is Hindi\n",
        "dataBase = DataProcessing(DATAPATH) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "    \n",
        "    Credits to Tensorflow.org and https://github.com/thushv89/attention_keras/blob/master/src/layers/attention.py\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state\n",
        "            inputs: (batchsize * 1 * de_in_dim)\n",
        "            states: (batchsize * 1 * de_latent_dim)\n",
        "            \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch size * en_seq_len * latent_dim\n",
        "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>', U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "metadata": {
        "id": "o9OmfUuF4Ve9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKyABHLuZqGx"
      },
      "source": [
        "## 3. Recurrent neural networks based model for sequence to sequence machine translation \n",
        "### 3.1 Seq2Seq **Translation** Model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT83-HQ3Z5fe"
      },
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import layers\n",
        " \n",
        "\n",
        "#from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Dense, Input, InputLayer, Flatten, Activation, LSTM, SimpleRNN, GRU, TimeDistributed\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import load_model, Sequential,  Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "\n",
        "class S2STranslation():\n",
        "\n",
        "    def __init__(self, modelConfigDict, srcChar2Int, tgtChar2Int, using_pretrained_model = False):\n",
        "        self.numEncoders = modelConfigDict[\"numEncoders\"]\n",
        "        self.cell_type = modelConfigDict[\"cell_type\"]\n",
        "        self.latentDim = modelConfigDict[\"latentDim\"]\n",
        "        self.dropout = modelConfigDict[\"dropout\"]\n",
        "        self.numDecoders = modelConfigDict[\"numDecoders\"]\n",
        "        self.hidden = modelConfigDict[\"hidden\"]\n",
        "        self.tgtChar2Int = tgtChar2Int\n",
        "        self.srcChar2Int = srcChar2Int\n",
        "\n",
        "    def build_configurable_model(self):       \n",
        "        if self.cell_type == \"RNN\":\n",
        "            # encoder\n",
        "            encoder_inputs = Input(shape=(None, len(self.srcChar2Int)))\n",
        "            encoder_outputs = encoder_inputs #was getting error that var referenced before assignment hence\n",
        "            for i in range(1, self.numEncoders + 1):\n",
        "                encoder = SimpleRNN(self.latentDim,return_state=True,return_sequences=True,dropout=self.dropout,name=\"encoder_1\")\n",
        "                encoder_outputs, state = encoder(encoder_inputs)\n",
        "            encoder_states = [state]\n",
        "\n",
        "            # decoder\n",
        "            decoder_inputs = Input(shape=(None, len(self.tgtChar2Int)))\n",
        "            decoder_outputs = decoder_inputs\n",
        "            for i in range(1, self.numDecoders + 1):\n",
        "                decoder = SimpleRNN(self.latentDim,return_sequences=True,return_state=True,dropout=self.dropout,name=\"decoder_1\")\n",
        "                decoder_outputs, _ = decoder(decoder_inputs, initial_state=encoder_states)\n",
        "\n",
        "            # dense layer\n",
        "            hidden = Dense(self.hidden, activation=\"relu\")\n",
        "            hidden_outputs = hidden(decoder_outputs)\n",
        "            decoder_dense = Dense(len(self.tgtChar2Int), activation=\"softmax\")\n",
        "            decoder_outputs = decoder_dense(hidden_outputs)\n",
        "            model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "            \n",
        "            return model\n",
        "        \n",
        "        elif self.cell_type == \"LSTM\":\n",
        "            # encoder\n",
        "            encoder_inputs = Input(shape=(None, len(self.srcChar2Int)))\n",
        "            encoder_outputs = encoder_inputs\n",
        "            for i in range(1, self.numEncoders + 1):\n",
        "                encoder = LSTM(\n",
        "                    self.latentDim,\n",
        "                    return_state=True,\n",
        "                    return_sequences=True,\n",
        "                    dropout=self.dropout,\n",
        "                )\n",
        "                encoder_outputs, state_h, state_c = encoder(encoder_outputs)\n",
        "            encoder_states = [state_h, state_c]\n",
        "\n",
        "            # decoder\n",
        "            decoder_inputs = Input(shape=(None, len(self.tgtChar2Int)))\n",
        "            decoder_outputs = decoder_inputs\n",
        "            for i in range(1, self.numDecoders + 1):\n",
        "                decoder = LSTM(\n",
        "                    self.latentDim,\n",
        "                    return_state=True,\n",
        "                    return_sequences=True,\n",
        "                    dropout=self.dropout,\n",
        "                )\n",
        "                decoder_outputs, _, _ = decoder(\n",
        "                    decoder_outputs, initial_state=encoder_states\n",
        "                )\n",
        "\n",
        "            # dense\n",
        "            hidden = Dense(self.hidden, activation=\"relu\")\n",
        "            hidden_outputs = hidden(decoder_outputs)\n",
        "            decoder_dense = Dense(len(self.tgtChar2Int), activation=\"softmax\")\n",
        "            decoder_outputs = decoder_dense(hidden_outputs)\n",
        "            model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "            \n",
        "            return model\n",
        "        \n",
        "        elif self.cell_type == \"GRU\":\n",
        "            # encoder\n",
        "            encoder_inputs = Input(shape=(None, len(self.srcChar2Int)))\n",
        "            #encoder_outputs = encoder_inputs\n",
        "            for i in range(1, self.numEncoders + 1):\n",
        "                encoder = GRU(\n",
        "                    self.latentDim,\n",
        "                    return_state=True,\n",
        "                    return_sequences=True,\n",
        "                    dropout=self.dropout,\n",
        "                )\n",
        "                encoder_outputs, state = encoder(encoder_inputs)\n",
        "            encoder_states = [state]\n",
        "\n",
        "            # decoder\n",
        "            decoder_inputs = Input(shape=(None, len(self.tgtChar2Int)))\n",
        "            #decoder_outputs = decoder_inputs\n",
        "            for i in range(1, self.numDecoders + 1):\n",
        "                decoder = GRU(\n",
        "                    self.latentDim,\n",
        "                    return_sequences=True,\n",
        "                    return_state=True,\n",
        "                    dropout=self.dropout,\n",
        "                )\n",
        "                decoder_outputs, _ = decoder(decoder_inputs, initial_state=encoder_states)\n",
        "\n",
        "            # dense\n",
        "            hidden = Dense(self.hidden, activation=\"relu\")\n",
        "            hidden_outputs = hidden(decoder_outputs)\n",
        "            decoder_dense = Dense(len(self.tgtChar2Int), activation=\"softmax\")\n",
        "            decoder_outputs = decoder_dense(hidden_outputs)\n",
        "            model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "            \n",
        "            return model\n",
        "            \n",
        "            \n",
        "    def build_attention_model(self):       \n",
        "        if self.cell_type == \"RNN\":\n",
        "            # encoder\n",
        "            encoder_inputs = Input(shape=(None, len(self.srcChar2Int)))\n",
        "            encoder_outputs = encoder_inputs\n",
        "            for i in range(1, self.numEncoders + 1):\n",
        "                encoder = SimpleRNN(\n",
        "                    self.latentDim,\n",
        "                    return_state=True,\n",
        "                    return_sequences=True,\n",
        "                    dropout=self.dropout,\n",
        "                )\n",
        "                encoder_outputs, state = encoder(encoder_inputs) \n",
        "                \n",
        "                if i == 1:\n",
        "                    encoder_first_outputs= encoder_outputs                  \n",
        "            encoder_states = [state]\n",
        "            \n",
        "\n",
        "            # decoder\n",
        "            decoder_inputs = Input(shape=(None, len(self.tgtChar2Int)))\n",
        "            decoder_outputs = decoder_inputs\n",
        "            for i in range(1, self.numDecoders + 1):\n",
        "                decoder = SimpleRNN(\n",
        "                    self.latentDim,\n",
        "                    return_sequences=True,\n",
        "                    return_state=True,\n",
        "                    dropout=self.dropout,\n",
        "                )\n",
        "                decoder_outputs, _ = decoder(decoder_inputs, initial_state=encoder_states)\n",
        "                \n",
        "                if i == self.numDecoders:\n",
        "                    decoder_first_outputs = decoder_outputs\n",
        "\n",
        "            attention_layer = AttentionLayer(name='attention_layer')\n",
        "            attention_out, attention_states = attention_layer([encoder_first_outputs, decoder_first_outputs])\n",
        "\n",
        "\n",
        "            decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attention_out])\n",
        "\n",
        "            # dense\n",
        "            hidden = Dense(self.hidden, activation=\"relu\")\n",
        "            hidden_time = TimeDistributed(hidden, name='time_distributed_layer')\n",
        "            hidden_outputs = hidden(decoder_concat_input)\n",
        "            decoder_dense = Dense(len(self.tgtChar2Int), activation=\"softmax\")\n",
        "            decoder_outputs = decoder_dense(hidden_outputs)\n",
        "            model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "            \n",
        "            return model\n",
        "        \n",
        "        elif self.cell_type == \"LSTM\":\n",
        "            # encoder\n",
        "            encoder_inputs = Input(shape=(None, len(self.srcChar2Int)))\n",
        "            encoder_outputs = encoder_inputs\n",
        "            for i in range(1, self.numEncoders + 1):\n",
        "                encoder = LSTM(\n",
        "                    self.latentDim,\n",
        "                    return_state=True,\n",
        "                    return_sequences=True,\n",
        "                    dropout=self.dropout,\n",
        "                )\n",
        "                encoder_outputs, state_h, state_c = encoder(encoder_outputs)\n",
        "                if i == 1:\n",
        "                    encoder_first_outputs= encoder_outputs                  \n",
        "         \n",
        "            encoder_states = [state_h, state_c]\n",
        "\n",
        "            # decoder\n",
        "            decoder_inputs = Input(shape=(None, len(self.tgtChar2Int)))\n",
        "            decoder_outputs = decoder_inputs\n",
        "            for i in range(1, self.numDecoders + 1):\n",
        "                decoder = LSTM(\n",
        "                    self.latentDim,\n",
        "                    return_state=True,\n",
        "                    return_sequences=True,\n",
        "                    dropout=self.dropout,\n",
        "                )\n",
        "                decoder_outputs, _, _ = decoder(\n",
        "                    decoder_outputs, initial_state=encoder_states\n",
        "                )\n",
        "                if i == self.numDecoders:\n",
        "                    decoder_first_outputs = decoder_outputs\n",
        "\n",
        "            attention_layer = AttentionLayer(name='attention_layer')\n",
        "            attention_out, attention_states = attention_layer([encoder_first_outputs, decoder_first_outputs])\n",
        "\n",
        "            decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attention_out])\n",
        "\n",
        "            # dense\n",
        "            hidden = Dense(self.hidden, activation=\"relu\")\n",
        "            hidden_time = TimeDistributed(hidden, name='time_distributed_layer')\n",
        "            hidden_outputs = hidden(decoder_concat_input)\n",
        "            decoder_dense = Dense(len(self.tgtChar2Int), activation=\"softmax\")\n",
        "            decoder_outputs = decoder_dense(hidden_outputs)\n",
        "            model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "            \n",
        "            return model\n",
        "        \n",
        "        elif self.cell_type == \"GRU\":\n",
        "            # encoder\n",
        "            encoder_inputs = Input(shape=(None, len(self.srcChar2Int)))\n",
        "            encoder_outputs = encoder_inputs\n",
        "            for i in range(1, self.numEncoders + 1):\n",
        "                encoder = GRU(\n",
        "                    self.latentDim,\n",
        "                    return_state=True,\n",
        "                    return_sequences=True,\n",
        "                    dropout=self.dropout,\n",
        "                )\n",
        "                encoder_outputs, state = encoder(encoder_inputs)\n",
        "\n",
        "                if i == 1:\n",
        "                    encoder_first_outputs= encoder_outputs                  \n",
        "         \n",
        "            encoder_states = [state]\n",
        "\n",
        "            # decoder\n",
        "            decoder_inputs = Input(shape=(None, len(self.tgtChar2Int)))\n",
        "            decoder_outputs = decoder_inputs\n",
        "            for i in range(1, self.numDecoders + 1):\n",
        "                decoder = GRU(\n",
        "                    self.latentDim,\n",
        "                    return_sequences=True,\n",
        "                    return_state=True,\n",
        "                    dropout=self.dropout,\n",
        "                )\n",
        "                decoder_outputs, _ = decoder(decoder_inputs, initial_state=encoder_states)\n",
        "                if i == self.numDecoders:\n",
        "                    decoder_first_outputs = decoder_outputs\n",
        "\n",
        "\n",
        "\n",
        "            attention_layer = AttentionLayer(name='attention_layer')\n",
        "            attention_out, attention_states = attention_layer([encoder_first_outputs, decoder_first_outputs])\n",
        "\n",
        "            decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attention_out])\n",
        "\n",
        "            # dense\n",
        "            hidden = Dense(self.hidden, activation=\"relu\")\n",
        "            hidden_time = TimeDistributed(hidden, name='time_distributed_layer')\n",
        "            hidden_outputs = hidden(decoder_concat_input)\n",
        "            decoder_dense = Dense(len(self.tgtChar2Int), activation=\"softmax\")\n",
        "            decoder_outputs = decoder_dense(hidden_outputs)\n",
        "            model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "            \n",
        "            return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjLa_0loaBPT"
      },
      "source": [
        "### 3.2 Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-B1RkORaHjO"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "#from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import RNN, LSTM, GRU, Dense\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def train():\n",
        "\n",
        "    config_defaults = {\n",
        "        \"cell_type\": \"RNN\",\n",
        "        \"latentDim\": 256,\n",
        "        \"hidden\": 128,\n",
        "        \"optimiser\": \"rmsprop\",\n",
        "        \"numEncoders\": 1,\n",
        "        \"numDecoders\": 1,\n",
        "        \"dropout\": 0.2,\n",
        "        \"epochs\": 1,\n",
        "        \"batch_size\": 64,\n",
        "    }\n",
        "    config_best = {\n",
        "        \"cell_type\": \"LSTM\",\n",
        "        \"latentDim\": 256,\n",
        "        \"hidden\": 64,\n",
        "        \"optimiser\": \"adam\",\n",
        "        \"numEncoders\": 2,\n",
        "        \"numDecoders\": 1,\n",
        "        \"dropout\": 0.1,\n",
        "        \"epochs\": 20,\n",
        "        \"batch_size\": 32,\n",
        "    }\n",
        "\n",
        "\n",
        "    wandb.init(config=config_best,  project=\"CS6910-Assignment-3\", entity=\"rashmi05pathak\")\n",
        "    config = wandb.config\n",
        "    wandb.run.name = (\n",
        "        str(config.cell_type)\n",
        "        + dataBase.source_lang\n",
        "        + str(config.numEncoders)\n",
        "        + \"_\"\n",
        "        + dataBase.target_lang\n",
        "        + \"_\"\n",
        "        + str(config.numDecoders)\n",
        "        + \"_\"\n",
        "        + config.optimiser\n",
        "        + \"_\"\n",
        "        + str(config.epochs)\n",
        "        + \"_\"\n",
        "        + str(config.dropout) \n",
        "        + \"_\"\n",
        "        + str(config.batch_size)\n",
        "        + \"_\"\n",
        "        + str(config.latentDim)\n",
        "    )\n",
        "    wandb.run.save()\n",
        "\n",
        "    modelInit = S2STranslation(config,srcChar2Int=dataBase.source_char2int, tgtChar2Int=dataBase.target_char2int)\n",
        "    \n",
        "    model = modelInit.build_configurable_model()\n",
        "    \n",
        "    model.summary()\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=config.optimiser,\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    earlystopping = EarlyStopping(\n",
        "        monitor=\"val_accuracy\", min_delta=0.01, patience=5, verbose=2, mode=\"auto\"\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        [dataBase.train_encoder_input, dataBase.train_decoder_input],\n",
        "        dataBase.train_decoder_target,\n",
        "        batch_size=config.batch_size,\n",
        "        epochs=config.epochs,\n",
        "        validation_data=([dataBase.val_encoder_input, dataBase.val_decoder_input], dataBase.val_decoder_target),\n",
        "        callbacks=[earlystopping, WandbCallback()],\n",
        "    )\n",
        "\n",
        "    model.save(os.path.join(\"./TrainedModels\", wandb.run.name))    \n",
        "    wandb.finish()\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyCe7u7IaaIN"
      },
      "source": [
        "Running the train function without sweep: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xeopy2fZacpG"
      },
      "source": [
        "'''    \n",
        "sweep_config = {\n",
        "    \"name\": \"Bayesian Sweep without attention\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
        "    \"parameters\": {\n",
        "        \n",
        "        \"cell_type\": {\"values\": [\"LSTM\"]},\n",
        "        \n",
        "        \"latentDim\": {\"values\": [256]},\n",
        "        \n",
        "        \"hidden\": {\"values\": [128, 64]},\n",
        "        \n",
        "        \"optimiser\": {\"values\": [\"rmsprop\", \"adam\"]},\n",
        "        \n",
        "        \"numEncoders\": {\"values\": [1, 2, 3]},\n",
        "        \n",
        "        \"numDecoders\": {\"values\": [1, 2, 3]},\n",
        "        \n",
        "        \"dropout\": {\"values\": [0.1, 0.2, 0.3]},\n",
        "        \n",
        "        \"epochs\": {\"values\": [5,10,15]},\n",
        "        \n",
        "        \"batch_size\": {\"values\": [32, 64]},\n",
        "    },\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910-Assignment-3\", entity=\"rashmi05pathak\")\n",
        "\n",
        "wandb.agent(sweep_id, train)\n",
        "\n",
        "'''\n",
        "model = train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "jIHEdjfudRzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_model(model,to_file='model.png',show_shapes = True)"
      ],
      "metadata": {
        "id": "G304nxv6gPH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[-1]"
      ],
      "metadata": {
        "id": "eHMt32G_b1eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[4]"
      ],
      "metadata": {
        "id": "WfBPjBMccAly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwIBB2t3ahQK"
      },
      "source": [
        "Running the wandb sweep: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbZXjvV9agMF"
      },
      "source": [
        "  \n",
        "sweep_config = {\n",
        "    \"name\": \"Bayesian Sweep with attention\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
        "    \"parameters\": {\n",
        "        \n",
        "        \"cell_type\": {\"values\": [\"RNN\", \"GRU\", \"LSTM\"]},\n",
        "        \n",
        "        \"latentDim\": {\"values\": [256]},\n",
        "        \n",
        "        \"hidden\": {\"values\": [128, 64]},\n",
        "        \n",
        "        \"optimiser\": {\"values\": [\"rmsprop\", \"adam\"]},\n",
        "        \n",
        "        \"numEncoders\": {\"values\": [1, 2, 3]},\n",
        "        \n",
        "        \"numDecoders\": {\"values\": [1, 2, 3]},\n",
        "        \n",
        "        \"dropout\": {\"values\": [0.1, 0.2, 0.3]},\n",
        "        \n",
        "        \"epochs\": {\"values\": [5,10,15, 20]},\n",
        "        \n",
        "        \"batch_size\": {\"values\": [32, 64]},\n",
        "    },\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910-Assignment-3\", entity=\"rashmi05pathak\")\n",
        "\n",
        "wandb.agent(sweep_id, train)\n",
        "\n",
        "#train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dense, Input, InputLayer, Flatten, Activation, LSTM, SimpleRNN, GRU, TimeDistributed, Concatenate\n",
        "\n",
        "\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "#By default source language is english and target lang is Hindi\n",
        "dataBase = DataProcessing(DATAPATH) \n",
        "\n",
        "config_best = {\n",
        "        \"cell_type\": \"LSTM\",\n",
        "        \"latentDim\": 256,\n",
        "        \"hidden\": 64,\n",
        "        \"optimiser\": \"adam\",\n",
        "        \"numEncoders\": 2,\n",
        "        \"numDecoders\": 1,\n",
        "        \"dropout\": 0.1,\n",
        "        \"epochs\": 20,\n",
        "        \"batch_size\": 32,\n",
        "    }\n",
        "\n",
        "config_defaults = {\n",
        "        \"cell_type\": \"RNN\",\n",
        "        \"latentDim\": 256,\n",
        "        \"hidden\": 128,\n",
        "        \"optimiser\": \"rmsprop\",\n",
        "        \"numEncoders\": 1,\n",
        "        \"numDecoders\": 1,\n",
        "        \"dropout\": 0.2,\n",
        "        \"epochs\": 1,\n",
        "        \"batch_size\": 64,\n",
        "    }\n",
        "    \n",
        "config_best_attention = {\n",
        "        \"cell_type\": \"RNN\",\n",
        "        \"latentDim\": 256,\n",
        "        \"hidden\": 16,\n",
        "        \"optimiser\": \"rmsprop\",\n",
        "        \"numEncoders\": 1,\n",
        "        \"numDecoders\": 1,\n",
        "        \"dropout\": 0.1,\n",
        "        \"epochs\": 10,\n",
        "        \"batch_size\": 32,\n",
        "    }\n",
        "    \n",
        "def test_model(\n",
        "    model,\n",
        "    attention = False\n",
        "):\n",
        "\n",
        "    if attention == False:\n",
        "        wandb.init(config=config_best,  project=\"CS6910-Assignment-3\", entity=\"rashmi05pathak\")\n",
        "        config = wandb.config\n",
        "        wandb.run.name = (\n",
        "            \"Inference_\" \n",
        "            + str(config.cell_type)\n",
        "            + dataBase.source_lang\n",
        "            + str(config.numEncoders)\n",
        "            + \"_\"\n",
        "            + dataBase.target_lang\n",
        "            + \"_\"\n",
        "            + str(config.numDecoders)\n",
        "            + \"_\"\n",
        "            + config.optimiser\n",
        "            + \"_\"\n",
        "            + str(config.epochs)\n",
        "            + \"_\"\n",
        "            + str(config.dropout) \n",
        "            + \"_\"\n",
        "            + str(config.batch_size)\n",
        "            + \"_\"\n",
        "            + str(config.latentDim)\n",
        "        )\n",
        "        wandb.run.save()\n",
        "\n",
        "\n",
        "        if config.cell_type == \"LSTM\":\n",
        "            encoder_inputs = model.input[0]\n",
        "            \n",
        "            if config.numEncoders == 1:\n",
        "                encoder_outputs, state_h_enc, state_c_enc = model.get_layer(name = \"lstm\").output \n",
        "            else:           \n",
        "                encoder_outputs, state_h_enc, state_c_enc = model.get_layer(name = \"lstm_\"+ str(config.numEncoders-1)).output\n",
        "\n",
        "            encoder_states = [state_h_enc, state_c_enc]\n",
        "            encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "            decoder_inputs = model.input[1]\n",
        "            decoder_state_input_h = Input(shape=(config.latentDim,), name=\"input_3\")\n",
        "            decoder_state_input_c = Input(shape=(config.latentDim,), name=\"input_4\")\n",
        "            decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "            decoder_lstm = model.layers[-3]\n",
        "            decoder_outputs, state_h_dec, state_c_dec = decoder_lstm( decoder_inputs, initial_state=decoder_states_inputs )\n",
        "            decoder_states = [state_h_dec, state_c_dec]\n",
        "            decoder_dense = model.layers[-2]\n",
        "            decoder_outputs = decoder_dense(decoder_outputs)\n",
        "            \n",
        "            decoder_dense = model.layers[-1]\n",
        "            decoder_outputs = decoder_dense(decoder_outputs)\n",
        "            decoder_model = Model(\n",
        "                [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        "            )\n",
        "        elif config.cell_type == \"GRU\" or config.cell_type == \"RNN\":\n",
        "            encoder_inputs = model.input[0]\n",
        "            if config.cell_type == \"GRU\":\n",
        "                if config.numEncoders == 1:\n",
        "                    encoder_outputs, state = model.get_layer(name = \"gru\").output\n",
        "                else:\n",
        "                    encoder_outputs, state = model.get_layer(name = \"gru_\"+ str(config.numEncoders-1)).output\n",
        "            else:\n",
        "                if config.numEncoders == 1:\n",
        "                    encoder_outputs, state = model.layers[2].output \n",
        "                else:\n",
        "                    encoder_outputs, state = model.get_layer(name = \"simple_rnn_\"+ str(config.numEncoders-1)).output\n",
        "\n",
        "            encoder_states = [state]\n",
        "\n",
        "            encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "            decoder_inputs = model.input[1]\n",
        "\n",
        "            decoder_state = Input(shape=(config.latentDim,))\n",
        "            decoder_states_inputs = [decoder_state]\n",
        "\n",
        "            decoder_gru = model.layers[-3]\n",
        "            (decoder_outputs, state,) = decoder_gru(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "            decoder_states = [state]\n",
        "            decoder_dense = model.layers[-2]\n",
        "            decoder_outputs = decoder_dense(decoder_outputs)\n",
        "            decoder_dense = model.layers[-1]\n",
        "            decoder_outputs = decoder_dense(decoder_outputs)\n",
        "            print(decoder_inputs)\n",
        "            print(decoder_outputs)\n",
        "            decoder_model = Model(\n",
        "                [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        "            )\n",
        "\n",
        "        def decode_sequence(input_seq):\n",
        "            # Encode the input as state vectors.\n",
        "            states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "            # Generate empty target sequence of length 1.\n",
        "            target_seq = np.zeros((1, 1, len(dataBase.target_char2int)))\n",
        "            # Populate the first character of target sequence with the start character.\n",
        "            target_seq[0, 0, dataBase.target_char2int[\"\\n\"]] = 1.0\n",
        "\n",
        "            # Sampling loop for a batch of sequences\n",
        "            # (to simplify, here we assume a batch of size 1).\n",
        "            stop_condition = False\n",
        "            decoded_sentence = \"\"\n",
        "            while not stop_condition:\n",
        "                if config.cell_type == \"LSTM\":\n",
        "                    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "                elif config.cell_type == \"RNN\" or config.cell_type == \"GRU\":\n",
        "                    states_value = states_value[0].reshape((1, 256))\n",
        "                    output_tokens, h = decoder_model.predict([target_seq] + [states_value])\n",
        "\n",
        "                # Sample a token\n",
        "                sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "                sampled_char = dataBase.target_int2char[sampled_token_index]\n",
        "                decoded_sentence += sampled_char\n",
        "\n",
        "                if sampled_char == \"\\n\" or len(decoded_sentence) > 25:\n",
        "                    stop_condition = True\n",
        "\n",
        "                \n",
        "                target_seq = np.zeros((1, 1, len(dataBase.target_char2int)))\n",
        "                target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "                # Update states\n",
        "                if config.cell_type == \"LSTM\":\n",
        "                    states_value = [h, c]\n",
        "                elif config.cell_type == \"RNN\" or config.cell_type == \"GRU\":\n",
        "                    states_value = [h]\n",
        "            return decoded_sentence\n",
        "\n",
        "        acc = 0\n",
        "        sourcelang = []\n",
        "        predictions = []\n",
        "        original = []\n",
        "        for i, row in dataBase.test.iterrows():\n",
        "            input_seq = dataBase.test_encoder_input[i : i + 1]\n",
        "            decoded_sentence = decode_sequence(input_seq)\n",
        "            og_tokens = [dataBase.target_char2int[x] for x in row[\"tgt\"]]\n",
        "            predicted_tokens = [dataBase.target_char2int[x] for x in decoded_sentence.rstrip(\"\\n\")]\n",
        "            sourcelang.append(row['src'])\n",
        "            original.append(row['tgt'])\n",
        "            predictions.append(decoded_sentence)\n",
        "\n",
        "            if og_tokens == predicted_tokens:\n",
        "                acc += 1\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Finished {i} examples\")\n",
        "                print(f\"Source: {row['src']}\")\n",
        "                print(f\"Original: {row['tgt']}\")\n",
        "                print(f\"Predicted: {decoded_sentence}\")\n",
        "                print(f\"Accuracy: {acc / (i+1)}\")\n",
        "                print(og_tokens)\n",
        "                print(predicted_tokens)\n",
        "                \n",
        "\n",
        "        print(f'Test Accuracy: {acc}')\n",
        "        wandb.log({'test_accuracy': acc / len(dataBase.test)})\n",
        "        wandb.finish()\n",
        "        return acc / len(dataBase.test), sourcelang, original, predictions"
      ],
      "metadata": {
        "id": "5ZRNNB7KfUY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc,sourcelang, original, predictions = test_model(model,attention = False)"
      ],
      "metadata": {
        "id": "zfVfZNwjlxpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dict2 = [{\"input\":sourcelang[i], \"true\": original[i], \"predicted\": predictions[i]} for i in range(len(sourcelang))] "
      ],
      "metadata": {
        "id": "FtR-W9bU6iM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dict2"
      ],
      "metadata": {
        "id": "ipyCf7ZYqall"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = pd.DataFrame(dict2)"
      ],
      "metadata": {
        "id": "REmS_RzPqQEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_predictions"
      ],
      "metadata": {
        "id": "U1dm4fvAqnoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAdZD_yk1ykO"
      },
      "source": [
        "Move the trained models to Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNbxAtkQ1yC2"
      },
      "source": [
        "config_best_attention2 = {\n",
        "        \"cell_type\": \"GRU\",\n",
        "        \"latentDim\": 256,\n",
        "        \"hidden\": 128,\n",
        "        \"optimiser\": \"rmsprop\",\n",
        "        \"numEncoders\": 1,\n",
        "        \"numDecoders\": 1,\n",
        "        \"dropout\": 0.2,\n",
        "        \"epochs\": 10,\n",
        "        \"batch_size\": 32,\n",
        "    }\n",
        "\n",
        "#testing the model with attention\n",
        "def test_model_with_attention(model,attention = True):\n",
        "    if attention == True:\n",
        "        wandb.init(config=config_best_attention2,  project=\"CS6910-Assignment-3\", entity=\"rashmi05patha\")\n",
        "        config = wandb.config\n",
        "        wandb.run.name = (\n",
        "            \"Inference_WithAttn_\" \n",
        "            + str(config.cell_type)\n",
        "            + dataBase.source_lang\n",
        "            + str(config.numEncoders)\n",
        "            + \"_\"\n",
        "            + dataBase.target_lang\n",
        "            + \"_\"\n",
        "            + str(config.numDecoders)\n",
        "            + \"_\"\n",
        "            + config.optimiser\n",
        "            + \"_\"\n",
        "            + str(config.epochs)\n",
        "            + \"_\"\n",
        "            + str(config.dropout) \n",
        "            + \"_\"\n",
        "            + str(config.batch_size)\n",
        "            + \"_\"\n",
        "            + str(config.latentDim)\n",
        "        )\n",
        "        wandb.run.save()\n",
        "\n",
        "\n",
        "        if config.cell_type == \"LSTM\":\n",
        "            encoder_inputs = model.input[0]\n",
        "            if config.numEncoders == 1:\n",
        "                encoder_outputs, state_h_enc, state_c_enc = model.get_layer(name = \"lstm\").output \n",
        "            else:           \n",
        "                encoder_outputs, state_h_enc, state_c_enc = model.get_layer(name = \"lstm_\"+ str(config.numEncoders-1)).output\n",
        "            encoder_first_outputs, _, _ = model.get_layer(name = \"lstm\").output\n",
        "            encoder_states = [state_h_enc, state_c_enc]\n",
        "            encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "            decoder_inputs = model.input[1]\n",
        "            decoder_state_input_h = Input(shape=(config.latentDim,), name=\"input_3\")\n",
        "            decoder_state_input_c = Input(shape=(config.latentDim,), name=\"input_4\")\n",
        "            decoder_hidden_state = Input(shape=(None,config[\"latentDim\"]), name = \"input_5\")\n",
        "            decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "          \n",
        "            decoder_lstm = model.get_layer(name = \"lstm_\"+ str(config.numEncoders + config.numDecoders -1))\n",
        "            decoder_outputs, state_h_dec, state_c_dec = decoder_lstm( decoder_inputs, initial_state=decoder_states_inputs )\n",
        "            decoder_states = [state_h_dec, state_c_dec]\n",
        "\n",
        "            attention_layer = model.get_layer(name = \"attention_layer\")\n",
        "            attention_out, attention_states = attention_layer([encoder_first_outputs, decoder_outputs])\n",
        "\n",
        "\n",
        "            decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attention_out])\n",
        "            \n",
        "            decoder_dense = model.layers[-2]\n",
        "            decoder_time = TimeDistributed(decoder_dense)\n",
        "            hidden_outputs = decoder_time(decoder_concat_input)\n",
        "            decoder_dense = model.layers[-1]\n",
        "            decoder_outputs = decoder_dense(hidden_outputs)\n",
        "\n",
        "            decoder_model = Model(inputs = [decoder_inputs] + [decoder_hidden_state , decoder_states_inputs], outputs = [decoder_outputs] + decoder_states)\n",
        "            \n",
        "        elif config.cell_type == \"GRU\" or config.cell_type == \"RNN\":\n",
        "            encoder_inputs = model.input[0]\n",
        "            if config.cell_type == \"GRU\":\n",
        "                if config.numEncoders == 1:\n",
        "                    encoder_outputs, state = model.get_layer(name = \"gru\").output\n",
        "                else:\n",
        "                    encoder_outputs, state = model.get_layer(name = \"gru_\"+ str(config.numEncoders-1)).output\n",
        "                encoder_first_outputs, _ = model.get_layer(name = \"gru\").output\n",
        "            else:\n",
        "                if config.numEncoders == 1:\n",
        "                    encoder_outputs, state = model.get_layer(name = \"simple_rnn\").output\n",
        "                else:\n",
        "                    encoder_outputs, state = model.get_layer(name = \"simple_rnn_\"+ str(config.numEncoders-1)).output\n",
        "                encoder_first_outputs, _ = model.get_layer(name = \"simple_rnn\").output\n",
        "            encoder_states = [state]\n",
        "\n",
        "            encoder_model = Model(encoder_inputs, outputs = [encoder_first_outputs, encoder_outputs] + encoder_states)\n",
        "\n",
        "            decoder_inputs = model.input[1]\n",
        "\n",
        "            decoder_state = Input(shape=(config.latentDim,), name=\"input_3\")\n",
        "            decoder_hidden_state = Input(shape=(None,config[\"latentDim\"]), name = \"input_4\")\n",
        "            decoder_states_inputs = [decoder_state]\n",
        "\n",
        "            if config.cell_type == \"GRU\":\n",
        "                decoder_gru = model.get_layer(name = \"gru_\"+ str(config.numEncoders + config.numDecoders -1))#model.layers[-3]\n",
        "                (decoder_outputs, state) = decoder_gru(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "                decoder_states = [state]\n",
        "\n",
        "            else:\n",
        "                decoder_gru = model.get_layer(name = \"simple_rnn_\"+ str(config.numEncoders + config.numDecoders -1))#model.layers[-3]\n",
        "                (decoder_outputs, state) = decoder_gru(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "                decoder_states = [state]\n",
        "\n",
        "                    \n",
        "            attention_layer = AttentionLayer(name='attention_layer')\n",
        "            attention_out, attention_states = attention_layer([decoder_hidden_state, decoder_outputs])\n",
        "\n",
        "            decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attention_out])\n",
        "\n",
        "            decoder_dense = model.layers[-2]\n",
        "            decoder_time = TimeDistributed(decoder_dense)\n",
        "            hidden_outputs = decoder_time(decoder_concat_input)\n",
        "            decoder_dense = model.layers[-1]\n",
        "            decoder_outputs = decoder_dense(hidden_outputs)\n",
        "\n",
        "            decoder_model = Model(inputs = [decoder_inputs] + [decoder_hidden_state , decoder_states_inputs], outputs = [decoder_outputs] + decoder_states)\n",
        "            \n",
        "        def decode_sequence(input_seq):\n",
        "            # Encode the input as state vectors.\n",
        "            encoder_first_outputs, _, states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "            # Generate empty target sequence of length 1.\n",
        "            target_seq = np.zeros((1, 1, len(dataBase.target_char2int)))\n",
        "            # Populate the first character of target sequence with the start character.\n",
        "            target_seq[0, 0, dataBase.target_char2int[\"\\n\"]] = 1.0\n",
        "\n",
        "            # Sampling loop for a batch of sequences\n",
        "            # (to simplify, here we assume a batch of size 1).\n",
        "            stop_condition = False\n",
        "            decoded_sentence = \"\"\n",
        "            attention_weights = []\n",
        "            while not stop_condition:\n",
        "                if config.cell_type == \"LSTM\":\n",
        "                    output_tokens, h, c = decoder_model.predict([target_seq, encoder_first_outputs] + states_value)\n",
        "                elif config.cell_type == \"RNN\" or config.cell_type == \"GRU\":\n",
        "                    states_value = states_value[0].reshape((1, config.latentDim))\n",
        "                    output_tokens, h = decoder_model.predict([target_seq] + [encoder_first_outputs] + [states_value])\n",
        "                #dec_ind = np.argmax(output_tokens, axis=-1)[0, 0]\n",
        "                #attention_weights.append((dec_ind, attn_states))\n",
        "                # Sample a token\n",
        "                sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "                sampled_char = dataBase.target_int2char[sampled_token_index]\n",
        "                decoded_sentence += sampled_char\n",
        "\n",
        "                # Exit condition: either hit max length\n",
        "                # or find stop character.\n",
        "                if sampled_char == \"\\n\" or len(decoded_sentence) > 25:\n",
        "                    stop_condition = True\n",
        "\n",
        "                # Update the target sequence (of length 1).\n",
        "                target_seq = np.zeros((1, 1, len(dataBase.target_char2int)))\n",
        "                target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "                # Update states\n",
        "                if config.cell_type == \"LSTM\":\n",
        "                    states_value = [h, c]\n",
        "                elif config.cell_type == \"RNN\" or config.cell_type == \"GRU\":\n",
        "                    states_value = [h]\n",
        "            return decoded_sentence #, attention_weights\n",
        "\n",
        "        acc = 0\n",
        "        sourcelang = []\n",
        "        predictions = []\n",
        "        original = []\n",
        "        for i, row in dataBase.test.iterrows():\n",
        "            input_seq = dataBase.test_encoder_input[i : i + 1]\n",
        "            decoded_sentence, attention_weights = decode_sequence(input_seq)\n",
        "            og_tokens = [dataBase.target_char2int[x] for x in row[\"tgt\"]]\n",
        "            predicted_tokens = [dataBase.target_char2int[x] for x in decoded_sentence.rstrip(\"\\n\")]\n",
        "            sourcelang.append(row['src'])\n",
        "            original.append(row['tgt'])\n",
        "            predictions.append(decoded_sentence)\n",
        "            \n",
        "            if og_tokens == predicted_tokens:\n",
        "                acc += 1\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Finished {i} examples\")\n",
        "                print(f\"Source: {row['src']}\")\n",
        "                print(f\"Original: {row['tgt']}\")\n",
        "                print(f\"Predicted: {decoded_sentence}\")\n",
        "                print(f\"Accuracy: {acc / (i+1)}\")\n",
        "                print(og_tokens)\n",
        "                print(predicted_tokens)\n",
        "                \n",
        "\n",
        "        print(f'Test Accuracy: {acc}')\n",
        "        wandb.log({'test_accuracy': acc / len(dataBase.test)})\n",
        "        wandb.finish()\n",
        "        return acc / len(dataBase.test) , sourcelang, original, predictions #, attention_weights_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIHCjgp62Cur"
      },
      "source": [
        "acc,sourcelang, original, predictions = test_model(model,attention = True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}